#!/usr/bin/env python3
# scripts/save_nbp_rates.py
# Rozszerzona wersja z migracjƒÖ plik√≥w umieszczonych w katalogach typu docs/exc/1, docs/exc/2, ...

from datetime import date, datetime, timedelta
from zoneinfo import ZoneInfo
import urllib.request
import urllib.error
import json
import os
import sys
import tempfile
import time
import gzip
import shutil
import re

TZ = "Europe/Warsaw"

BASE_OUT_DIR = os.path.join("docs", "exc")

# Domy≈õlny rok startowy: 2002. Nadpisz przez START_YEAR w env, np. START_YEAR=2010
START_YEAR = int(os.getenv("START_YEAR", "2002"))
START_DATE = date(START_YEAR, 1, 1)

CHUNK_DAYS = 93

BACKFILL_MARKER = os.path.join(BASE_OUT_DIR, ".backfill_done")
LAST_MARKER = os.path.join(BASE_OUT_DIR, ".last")

BASE_TABLE_URL = (
    "https://api.nbp.pl/api/exchangerates/tables/A/"
    "{start}/{end}/?format=json"
)
SINGLE_DAY_URL = (
    "https://api.nbp.pl/api/exchangerates/tables/A/"
    "{date}/?format=json"
)

HEADERS = {
    "User-Agent": "nbp-exchange-rates-fetcher/1.0"
}

DATE_FILENAME_RE = re.compile(r"^(\d{2})_(\d{2})_(\d{4})\.json\.gz$")


# --- util

def ensure_base_dir():
    os.makedirs(BASE_OUT_DIR, exist_ok=True)


def append_last_marker(path):
    try:
        os.makedirs(os.path.dirname(LAST_MARKER), exist_ok=True)
        with open(LAST_MARKER, "a", encoding="utf-8") as f:
            now_str = datetime.now(ZoneInfo(TZ)).strftime("%Y-%m-%d %H:%M:%S")
            f.write(f"{now_str}: {path}\n")
    except Exception as e:
        print("‚ùå B≈ÇƒÖd zapisu .last:", e)


def path_for_date(d: date):
    """
    Zwraca ≈õcie≈ºkƒô docs/exc/<YEAR>/<dd_mm_YYYY>.json.gz i tworzy katalog YEAR je≈õli potrzeba.
    """
    year_dir = os.path.join(BASE_OUT_DIR, str(d.year))
    os.makedirs(year_dir, exist_ok=True)
    filename = d.strftime("%d_%m_%Y.json.gz")
    return os.path.join(year_dir, filename)


def write_json_gz_atomic(path, data):
    """
    Zapisuje JSON skompresowany gzip atomowo (tmp -> os.replace).
    Zwraca True je≈õli OK.
    """
    dirn = os.path.dirname(path)
    os.makedirs(dirn, exist_ok=True)
    # przygotuj zawarto≈õƒá
    try:
        payload_bytes = json.dumps(
            data,
            ensure_ascii=False,
            separators=(",", ":")
        ).encode("utf-8")
    except Exception as e:
        print("‚ùå B≈ÇƒÖd serializacji JSON:", e)
        return False

    fd, tmp_path = tempfile.mkstemp(suffix=".json.gz", dir=dirn)
    os.close(fd)
    try:
        with gzip.open(tmp_path, "wb") as gz:
            gz.write(payload_bytes)
        os.replace(tmp_path, path)
        print("‚úÖ Zapisano:", path)
        return True
    except Exception as e:
        print("‚ùå B≈ÇƒÖd zapisu (gzip):", e)
        try:
            os.remove(tmp_path)
        except Exception:
            pass
        return False


# http_get z retry/backoff. Zwraca tre≈õƒá (string) lub obiekt urllib.error.HTTPError lub inny Exception
def http_get(url, retries=3, backoff_base=0.5, timeout=60):
    attempt = 0
    while True:
        attempt += 1
        req = urllib.request.Request(url, headers=HEADERS)
        try:
            with urllib.request.urlopen(req, timeout=timeout) as resp:
                raw = resp.read()
                charset = resp.headers.get_content_charset() or "utf-8"
                return raw.decode(charset)
        except urllib.error.HTTPError as e:
            # 404 chcemy zwr√≥ciƒá natychmiast (kto≈õ sprawdza resp.code == 404)
            if e.code == 404:
                return e
            # dla b≈Çƒôd√≥w serwera mo≈ºemy spr√≥bowaƒá retry
            if 500 <= e.code < 600 and attempt <= retries:
                wait = backoff_base * (2 ** (attempt - 1))
                print(f"‚ö† HTTPError {e.code}, pr√≥ba {attempt}/{retries}. Czekam {wait}s i retry...")
                time.sleep(wait)
                continue
            return e
        except Exception as e:
            # transient network error - retry limited times
            if attempt <= retries:
                wait = backoff_base * (2 ** (attempt - 1))
                print(f"‚ö† B≈ÇƒÖd sieci ({e}), pr√≥ba {attempt}/{retries}. Czekam {wait}s i retry...")
                time.sleep(wait)
                continue
            print("‚ùå HTTP:", e)
            return e


# defensywna funkcja przetwarzajaca pojedy≈Ñczy wpis
def process_table_entry(entry):
    # defensywne pobranie p√≥l
    eff_date = None
    if isinstance(entry, dict):
        eff_date = entry.get("effectiveDate") or entry.get("effective_date")
        rates = entry.get("rates", []) if isinstance(entry, dict) else []
    else:
        print("‚ö† Nieoczekiwany entry (nie dict) ‚Äî pomijam:", entry)
        return False

    if not eff_date:
        print("‚ö† Brak pola effectiveDate w entry, pomijam:", entry)
        return False

    try:
        d = datetime.strptime(eff_date, "%Y-%m-%d").date()
    except Exception as e:
        print("‚ùå Nieprawid≈Çowy format daty:", eff_date, e)
        return False

    out_path = path_for_date(d)

    if os.path.exists(out_path):
        append_last_marker(out_path)
        return True

    rates_list = []
    for r in rates:
        if not isinstance(r, dict):
            print("‚ö† Nieoczekiwany element w rates (nie dict) ‚Äî pomijam:", r)
            continue

        code = r.get("code")
        currency = r.get("currency") or r.get("name") or None

        rate_entry = {}
        if currency is not None:
            rate_entry["currency"] = currency
        if code is not None:
            rate_entry["code"] = code
        if "mid" in r:
            rate_entry["mid"] = r["mid"]
        if "bid" in r:
            rate_entry["bid"] = r["bid"]
        if "ask" in r:
            rate_entry["ask"] = r["ask"]

        if not rate_entry:
            print("‚ö† Pusty/nieu≈ºyteczny rate_entry ‚Äî pomijam:", r)
            continue

        rates_list.append(rate_entry)

    payload = {
        "date": eff_date,
        "rates": rates_list,
    }

    if write_json_gz_atomic(out_path, payload):
        append_last_marker(out_path)
        return True
    return False


def fetch_range(start_d: date, end_d: date):
    url = BASE_TABLE_URL.format(
        start=start_d.isoformat(),
        end=end_d.isoformat()
    )
    resp = http_get(url)
    if isinstance(resp, Exception):
        return None
    try:
        return json.loads(resp)
    except Exception:
        return None


def backfill():
    print("üîÅ BACKFILL od", START_DATE.isoformat())
    cur = START_DATE
    today = date.today()
    bad_dir = os.path.join(BASE_OUT_DIR, "bad_entries")
    os.makedirs(bad_dir, exist_ok=True)

    while cur <= today:
        chunk_end = min(cur + timedelta(days=CHUNK_DAYS - 1), today)
        print(f"üì• Pobieram zakres: {cur.isoformat()} ‚Äî {chunk_end.isoformat()}")
        data = fetch_range(cur, chunk_end)
        if data:
            for entry in data:
                try:
                    process_table_entry(entry)
                except Exception as e:
                    # nie przerywamy backfilla ‚Äî zapisujemy problematyczny wpis do folderu bad_entries
                    print("‚ùå B≈ÇƒÖd przetwarzania wpisu (zapisujƒô do bad_entries):", e)
                    bad_path = os.path.join(
                        bad_dir,
                        "bad_" + datetime.utcnow().isoformat().replace(":", "_") + ".json"
                    )
                    try:
                        with open(bad_path, "w", encoding="utf-8") as bf:
                            json.dump(entry, bf, ensure_ascii=False, indent=2)
                        print("‚Ñπ Zapisano problematyczny wpis:", bad_path)
                    except Exception as e2:
                        print("‚ùå Nie uda≈Ço siƒô zapisaƒá problematycznego wpisu:", e2)
        else:
            print(f"‚ö† Brak danych dla zakresu {cur.isoformat()} ‚Äî {chunk_end.isoformat()}")

        cur = chunk_end + timedelta(days=1)

    try:
        with open(BACKFILL_MARKER, "w", encoding="utf-8") as f:
            f.write(datetime.utcnow().isoformat())
    except Exception as e:
        print("‚ùå Nie uda≈Ço siƒô zapisaƒá BACKFILL_MARKER:", e)
    print("‚úÖ BACKFILL ZAKO≈ÉCZONY")


def fetch_recent_and_today(today: date, lookback_days: int = 7):
    start = today - timedelta(days=lookback_days - 1)
    print(f"üîé Pr√≥ba pobrania zakresu {start.isoformat()} ‚Äî {today.isoformat()}")
    data = fetch_range(start, today)
    if data:
        print(f"‚Ñπ Znalaz≈Çem {len(data)} wpis√≥w w zakresie, przetwarzam...")
        for entry in data:
            process_table_entry(entry)
        return True

    print("‚Ñπ Zakres nic nie zwr√≥ci≈Ç ‚Äî pr√≥bujƒô pojedynczych dni wstecz")
    for i in range(0, lookback_days):
        d = today - timedelta(days=i)
        url = SINGLE_DAY_URL.format(date=d.isoformat())
        resp = http_get(url)
        if isinstance(resp, urllib.error.HTTPError):
            # 404 -> brak tabeli w tym dniu (weekend/≈õwiƒôto)
            if resp.code == 404:
                print(f"‚Ñπ {d.isoformat()}: brak (404)")
                continue
            print(f"‚ùå B≈ÇƒÖd HTTP dla {d.isoformat()}: {resp}")
            return False
        if isinstance(resp, Exception):
            print("‚ùå B≈ÇƒÖd przy pobieraniu:", resp)
            return False
        try:
            data = json.loads(resp)
        except Exception as e:
            print("‚ùå Nie uda≈Ço siƒô zdekodowaƒá JSON:", e)
            return False
        if data:
            print(f"‚Ñπ {d.isoformat()}: znaleziono dane, przetwarzam...")
            for entry in data:
                process_table_entry(entry)
            return True

    print(f"‚Ñπ Brak kurs√≥w w ostatnich {lookback_days} dniach (weekend/≈õwiƒôta).")
    return True


# --- Nowa funkcja migracji

def migrate_misplaced_files():
    """
    Przeszukuje BASE_OUT_DIR i przenosi pliki o nazwie dd_mm_YYYY.json.gz,
    kt√≥re le≈ºƒÖ w katalogach nie-bƒôdƒÖcych katalogami roku (np. '1', '2', '3', ...),
    do katalogu docs/exc/<YYYY>/<dd_mm_YYYY>.json.gz

    Zachowanie przy konflikcie:
      - je≈õli plik docelowy nie istnieje -> move
      - je≈õli istnieje i ma tƒô samƒÖ wielko≈õƒá -> usu≈Ñ ≈∫r√≥d≈Ço (uznajemy za duplikat)
      - je≈õli istnieje i r√≥≈ºna wielko≈õƒá -> przenie≈õ ≈∫r√≥d≈Ço, dodajƒÖc suffix "-conflict-<timestamp>"
    """
    print("üîß Sprawdzam i migrujƒô pliki z b≈Çƒôdnych katalog√≥w (je≈õli wystƒôpujƒÖ)...")
    if not os.path.isdir(BASE_OUT_DIR):
        print("‚Ñπ Brak katalogu bazowego, pomijam migracjƒô.")
        return

    for entry in os.listdir(BASE_OUT_DIR):
        entry_path = os.path.join(BASE_OUT_DIR, entry)
        # Je≈õli to katalog-rok (4 cyfry), pomi≈Ñ
        if not os.path.isdir(entry_path):
            continue
        if re.fullmatch(r"\d{4}", entry):
            # katalog prawdopodobnie prawid≈Çowy: "2023", "2024"
            continue

        # Przeszukujemy katalog entry_path w poszukiwaniu plik√≥w pasujƒÖcych do nazwy daty
        moved_any = False
        for root, dirs, files in os.walk(entry_path):
            for fname in files:
                m = DATE_FILENAME_RE.match(fname)
                if not m:
                    continue
                day, month, year = m.groups()
                try:
                    # validacja daty
                    _ = date(int(year), int(month), int(day))
                except Exception:
                    print(f"‚ö† Nieprawid≈Çowa data w nazwie pliku {fname} ‚Äî pomijam.")
                    continue

                src = os.path.join(root, fname)
                dest_dir = os.path.join(BASE_OUT_DIR, year)
                os.makedirs(dest_dir, exist_ok=True)
                dest = os.path.join(dest_dir, fname)

                if not os.path.exists(dest):
                    try:
                        shutil.move(src, dest)
                        print(f"‚û° Przeniesiono: {src} -> {dest}")
                        moved_any = True
                    except Exception as e:
                        print(f"‚ùå Nie uda≈Ço siƒô przenie≈õƒá {src} -> {dest}: {e}")
                else:
                    try:
                        src_sz = os.path.getsize(src)
                        dest_sz = os.path.getsize(dest)
                        if src_sz == dest_sz:
                            # duplikat -> usu≈Ñ ≈∫r√≥d≈Ço
                            os.remove(src)
                            print(f"‚Ñπ Duplikat (ten sam rozmiar) ‚Äî usuniƒôto ≈∫r√≥d≈Ço: {src}")
                            moved_any = True
                        else:
                            ts = datetime.utcnow().strftime("%Y%m%d%H%M%S")
                            new_name = f"{fname[:-7]}-conflict-{ts}.json.gz"  # fname[:-7] to dd_mm_YYYY
                            new_dest = os.path.join(dest_dir, new_name)
                            shutil.move(src, new_dest)
                            print(f"‚ö† Konflikt rozmiaru ‚Äî przeniesiono jako: {new_dest}")
                            moved_any = True
                    except Exception as e:
                        print(f"‚ùå B≈ÇƒÖd przy obs≈Çudze konfliktu dla {src}: {e}")

        # po przej≈õciu po katalogu spr√≥buj usunƒÖƒá pusty katalogy
        # (tylko je≈õli emptiness; nie usuwamy katalogu je≈õli co≈õ pozosta≈Ço)
        for root, dirs, files in os.walk(entry_path, topdown=False):
            # usu≈Ñ pliki tymczasowe (opcjonalnie) ‚Äî tu pomijamy
            if not os.listdir(root):
                try:
                    os.rmdir(root)
                    print(f"üßπ Usuniƒôto pusty katalog: {root}")
                except Exception:
                    pass

        if moved_any:
            print(f"‚úÖ Migracja z katalogu {entry_path} zako≈Ñczona.")
        else:
            # brak plik√≥w do migracji
            # (mo≈ºemy usunƒÖƒá puste katalogi powy≈ºej, ju≈º zrobione)
            pass

    print("üîß Migracja zako≈Ñczona.")


def main():
    ensure_base_dir()
    # wykonaj migracjƒô starych plik√≥w (je≈õli jakie≈õ sƒÖ w niew≈Ça≈õciwych folderach)
    try:
        migrate_misplaced_files()
    except Exception as e:
        print("‚ùå B≈ÇƒÖd podczas migracji plik√≥w:", e)

    today = datetime.now(ZoneInfo(TZ)).date()
    if not os.path.exists(BACKFILL_MARKER):
        backfill()
    else:
        print("‚úî Backfill ju≈º wykonany")
    fetch_recent_and_today(today, lookback_days=7)
    sys.exit(0)


if __name__ == "__main__":
    main()
